{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python2, python3\n",
    "\"\"\"Computes rouge scores between two text blobs.\n",
    "\n",
    "Implementation replicates the functionality in the original ROUGE package. See:\n",
    "\n",
    "Lin, Chin-Yew. ROUGE: a Package for Automatic Evaluation of Summaries. In\n",
    "Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004),\n",
    "Barcelona, Spain, July 25 - 26, 2004.\n",
    "\n",
    "Default options are equivalent to running:\n",
    "ROUGE-1.5.5.pl -e data -n 2 -a settings.xml\n",
    "\n",
    "Or with use_stemmer=True:\n",
    "ROUGE-1.5.5.pl -m -e data -n 2 -a settings.xml\n",
    "\n",
    "In these examples settings.xml lists input files and formats.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import re\n",
    "\n",
    "from nltk.stem import porter\n",
    "import six\n",
    "from six.moves import map\n",
    "from six.moves import range\n",
    "from rouge_score import scoring\n",
    "# from rouge_score import tokenize\n",
    "\n",
    "\n",
    "class RougeScorer(scoring.BaseScorer):\n",
    "  \"\"\"Calculate rouges scores between two blobs of text.\n",
    "\n",
    "  Sample usage:\n",
    "    scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                          'The quick brown dog jumps on the log.')\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, rouge_types, use_stemmer=False):\n",
    "    \"\"\"Initializes a new RougeScorer.\n",
    "\n",
    "    Valid rouge types that can be computed are:\n",
    "      rougen (e.g. rouge1, rouge2): n-gram based scoring.\n",
    "      rougeL: Longest common subsequence based scoring.\n",
    "\n",
    "    Args:\n",
    "      rouge_types: A list of rouge types to calculate.\n",
    "      use_stemmer: Bool indicating whether Porter stemmer should be used to\n",
    "        strip word suffixes to improve matching.\n",
    "    Returns:\n",
    "      A dict mapping rouge types to Score tuples.\n",
    "    \"\"\"\n",
    "\n",
    "    self.rouge_types = rouge_types\n",
    "    self._stemmer = porter.PorterStemmer() if use_stemmer else None\n",
    "\n",
    "  def score(self, target, prediction):\n",
    "    \"\"\"Calculates rouge scores between the target and prediction.\n",
    "\n",
    "    Args:\n",
    "      target: Text containing the target (ground truth) text.\n",
    "      prediction: Text containing the predicted text.\n",
    "    Returns:\n",
    "      A dict mapping each rouge type to a Score object.\n",
    "    Raises:\n",
    "      ValueError: If an invalid rouge type is encountered.\n",
    "    \"\"\"\n",
    "\n",
    "    target_tokens = tokenize(target, self._stemmer)\n",
    "    prediction_tokens = tokenize(prediction, self._stemmer)\n",
    "    result = {}\n",
    "\n",
    "    for rouge_type in self.rouge_types:\n",
    "      if rouge_type == \"rougeL\":\n",
    "        # Rouge from longest common subsequences.\n",
    "        scores = _score_lcs(target_tokens, prediction_tokens)\n",
    "      elif rouge_type == \"rougeLsum\":\n",
    "        # Note: Does not support multi-line text.\n",
    "        def get_sents(text):\n",
    "          # Assume sentences are separated by newline.\n",
    "          sents = six.ensure_str(text).split(\"\\n\")\n",
    "          sents = [x for x in sents if len(x)]\n",
    "          return sents\n",
    "\n",
    "        target_tokens_list = [\n",
    "            tokenize(s, self._stemmer) for s in get_sents(target)]\n",
    "        prediction_tokens_list = [\n",
    "            tokenize(s, self._stemmer) for s in get_sents(prediction)]\n",
    "        scores = _summary_level_lcs(target_tokens_list,\n",
    "                                    prediction_tokens_list)\n",
    "      elif re.match(r\"rouge[0-9]$\", six.ensure_str(rouge_type)):\n",
    "        # Rouge from n-grams.\n",
    "        n = int(rouge_type[5:])\n",
    "        if n <= 0:\n",
    "          raise ValueError(\"rougen requires positive n: %s\" % rouge_type)\n",
    "        target_ngrams = _create_ngrams(target_tokens, n)\n",
    "        prediction_ngrams = _create_ngrams(prediction_tokens, n)\n",
    "        scores = _score_ngrams(target_ngrams, prediction_ngrams)\n",
    "      else:\n",
    "        raise ValueError(\"Invalid rouge type: %s\" % rouge_type)\n",
    "      result[rouge_type] = scores\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def _create_ngrams(tokens, n):\n",
    "  \"\"\"Creates ngrams from the given list of tokens.\n",
    "\n",
    "  Args:\n",
    "    tokens: A list of tokens from which ngrams are created.\n",
    "    n: Number of tokens to use, e.g. 2 for bigrams.\n",
    "  Returns:\n",
    "    A dictionary mapping each bigram to the number of occurrences.\n",
    "  \"\"\"\n",
    "\n",
    "  ngrams = collections.Counter()\n",
    "  for ngram in (tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)):\n",
    "    ngrams[ngram] += 1\n",
    "  return ngrams\n",
    "\n",
    "\n",
    "def _score_lcs(target_tokens, prediction_tokens):\n",
    "  \"\"\"Computes LCS (Longest Common Subsequence) rouge scores.\n",
    "\n",
    "  Args:\n",
    "    target_tokens: Tokens from the target text.\n",
    "    prediction_tokens: Tokens from the predicted text.\n",
    "  Returns:\n",
    "    A Score object containing computed scores.\n",
    "  \"\"\"\n",
    "\n",
    "  if not target_tokens or not prediction_tokens:\n",
    "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
    "\n",
    "  # Compute length of LCS from the bottom up in a table (DP appproach).\n",
    "  lcs_table = _lcs_table(target_tokens, prediction_tokens)\n",
    "  lcs_length = lcs_table[-1][-1]\n",
    "\n",
    "  precision = lcs_length / len(prediction_tokens)\n",
    "  recall = lcs_length / len(target_tokens)\n",
    "  fmeasure = scoring.fmeasure(precision, recall)\n",
    "\n",
    "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n",
    "\n",
    "\n",
    "def _lcs_table(ref, can):\n",
    "  \"\"\"Create 2-d LCS score table.\"\"\"\n",
    "  rows = len(ref)\n",
    "  cols = len(can)\n",
    "  lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)]\n",
    "  for i in range(1, rows + 1):\n",
    "    for j in range(1, cols + 1):\n",
    "      if ref[i - 1] == can[j - 1]:\n",
    "        lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1\n",
    "      else:\n",
    "        lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])\n",
    "  return lcs_table\n",
    "\n",
    "\n",
    "def _backtrack_norec(t, ref, can):\n",
    "  \"\"\"Read out LCS.\"\"\"\n",
    "  i = len(ref)\n",
    "  j = len(can)\n",
    "  lcs = []\n",
    "  while i > 0 and j > 0:\n",
    "    if ref[i - 1] == can[j - 1]:\n",
    "      lcs.insert(0, i-1)\n",
    "      i -= 1\n",
    "      j -= 1\n",
    "    elif t[i][j - 1] > t[i - 1][j]:\n",
    "      j -= 1\n",
    "    else:\n",
    "      i -= 1\n",
    "  return lcs\n",
    "\n",
    "\n",
    "def _summary_level_lcs(ref_sent, can_sent):\n",
    "  \"\"\"ROUGE: Summary-level LCS, section 3.2 in ROUGE paper.\n",
    "\n",
    "  Args:\n",
    "    ref_sent: list of tokenized reference sentences\n",
    "    can_sent: list of tokenized candidate sentences\n",
    "\n",
    "  Returns:\n",
    "    summary level ROUGE score\n",
    "  \"\"\"\n",
    "  if not ref_sent or not can_sent:\n",
    "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
    "\n",
    "  m = sum(map(len, ref_sent))\n",
    "  n = sum(map(len, can_sent))\n",
    "  if not n or not m:\n",
    "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
    "\n",
    "  # get token counts to prevent double counting\n",
    "  token_cnts_r = collections.Counter()\n",
    "  token_cnts_c = collections.Counter()\n",
    "  for s in ref_sent:\n",
    "    # s is a list of tokens\n",
    "    token_cnts_r.update(s)\n",
    "  for s in can_sent:\n",
    "    token_cnts_c.update(s)\n",
    "\n",
    "  hits = 0\n",
    "  for r in ref_sent:\n",
    "    lcs = _union_lcs(r, can_sent)\n",
    "    # Prevent double-counting:\n",
    "    # The paper describes just computing hits += len(_union_lcs()),\n",
    "    # but the implementation prevents double counting. We also\n",
    "    # implement this as in version 1.5.5.\n",
    "    for t in lcs:\n",
    "      if token_cnts_c[t] > 0 and token_cnts_r[t] > 0:\n",
    "        hits += 1\n",
    "        token_cnts_c[t] -= 1\n",
    "        token_cnts_r[t] -= 1\n",
    "\n",
    "  recall = hits / m\n",
    "  precision = hits / n\n",
    "  fmeasure = scoring.fmeasure(precision, recall)\n",
    "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n",
    "\n",
    "\n",
    "def _union_lcs(ref, c_list):\n",
    "  \"\"\"Find union LCS between a ref sentence and list of candidate sentences.\n",
    "\n",
    "  Args:\n",
    "    ref: list of tokens\n",
    "    c_list: list of list of indices for LCS into reference summary\n",
    "\n",
    "  Returns:\n",
    "    List of tokens in ref representing union LCS.\n",
    "  \"\"\"\n",
    "  lcs_list = [lcs_ind(ref, c) for c in c_list]\n",
    "  return [ref[i] for i in _find_union(lcs_list)]\n",
    "\n",
    "\n",
    "def _find_union(lcs_list):\n",
    "  \"\"\"Finds union LCS given a list of LCS.\"\"\"\n",
    "  return sorted(list(set().union(*lcs_list)))\n",
    "\n",
    "\n",
    "def lcs_ind(ref, can):\n",
    "  \"\"\"Returns one of the longest lcs.\"\"\"\n",
    "  t = _lcs_table(ref, can)\n",
    "  return _backtrack_norec(t, ref, can)\n",
    "\n",
    "\n",
    "def _score_ngrams(target_ngrams, prediction_ngrams):\n",
    "  \"\"\"Compute n-gram based rouge scores.\n",
    "\n",
    "  Args:\n",
    "    target_ngrams: A Counter object mapping each ngram to number of\n",
    "      occurrences for the target text.\n",
    "    prediction_ngrams: A Counter object mapping each ngram to number of\n",
    "      occurrences for the prediction text.\n",
    "  Returns:\n",
    "    A Score object containing computed scores.\n",
    "  \"\"\"\n",
    "\n",
    "  intersection_ngrams_count = 0\n",
    "  for ngram in six.iterkeys(target_ngrams):\n",
    "    intersection_ngrams_count += min(target_ngrams[ngram],\n",
    "                                     prediction_ngrams[ngram])\n",
    "  target_ngrams_count = sum(target_ngrams.values())\n",
    "  prediction_ngrams_count = sum(prediction_ngrams.values())\n",
    "\n",
    "  precision = intersection_ngrams_count / max(prediction_ngrams_count, 1)\n",
    "  recall = intersection_ngrams_count / max(target_ngrams_count, 1)\n",
    "  fmeasure = scoring.fmeasure(precision, recall)\n",
    "\n",
    "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text, stemmer):\n",
    "  \"\"\"Tokenize input text into a list of tokens.\n",
    "\n",
    "  This approach aims to replicate the approach taken by Chin-Yew Lin in\n",
    "  the original ROUGE implementation.\n",
    "\n",
    "  Args:\n",
    "    text: A text blob to tokenize.\n",
    "    stemmer: An optional stemmer.\n",
    "\n",
    "  Returns:\n",
    "    A list of string tokens extracted from input text.\n",
    "  \"\"\"\n",
    "\n",
    "  # Convert everything to lowercase.\n",
    "  text = text.lower()\n",
    "  # Replace any non-alpha-numeric characters with spaces.\n",
    "  # text = re.sub(r\"[^a-z0-9]+\", \" \", six.ensure_str(text))\n",
    "  # ^ [ / u4E00 - / u9FFF]+$\n",
    "  tokens = re.split(r\"\\s+\", text)\n",
    "\n",
    "  # print(tokens)\n",
    "  # print(1)\n",
    "  if stemmer:\n",
    "    # Only stem words more than 3 characters long.\n",
    "    tokens = [stemmer.stem(x) if len(x) > 3 else x for x in tokens]\n",
    "\n",
    "  # print(tokens)\n",
    "  # print(2)\n",
    "  # print(six.ensure_str(x))\n",
    "  # One final check to drop any empty or invalid tokens.\n",
    "  # tokens = [x for x in tokens if re.match(r\"^[/u4E00 - /u9FFF]+$\", six.ensure_str(x))]\n",
    "  # tokens = [x for x in tokens if re.match(r\"^[a-z0-9]+$\", six.ensure_str(x))]\n",
    "  tokens=[x for x in tokens if six.ensure_str]\n",
    "\n",
    "\n",
    "  # print(tokens)\n",
    "  # print(3)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(precision=0.5, recall=0.5, fmeasure=0.5)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from rouge_score import rouge_scorer\n",
    "# 我们只计算 rouge1 就行\n",
    "scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores =  scorer.score('武汉 大学',\n",
    "                          '武汉 测绘')\n",
    "# score =rouge_scorer. rougeEvaluationScore(candidate,references)\n",
    "scores['rouge1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                          'The quick brown dog jumps on the log.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765),\n",
       " 'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'tokenize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-de3061179150>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m scores = scorer.score('武汉 大学',\n\u001b[1;32m----> 2\u001b[1;33m                           '武汉 测绘')\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-e3d78dd2194e>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, target, prediction)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \"\"\"\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0mtarget_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stemmer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0mprediction_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stemmer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'tokenize'"
     ]
    }
   ],
   "source": [
    "scores = scorer.score('武汉 大学',\n",
    "                          '武汉 测绘')\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
